apiVersion: v1
kind: ServiceAccount
metadata:
  name: vector-logs
  namespace: unkey
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vector-logs
rules:
  - apiGroups: [""]
    resources: ["namespaces", "pods", "nodes"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vector-logs
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: vector-logs
subjects:
  - kind: ServiceAccount
    name: vector-logs
    namespace: unkey
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vector-logs-config
  namespace: unkey
data:
  vector.toml: |
    # Vector configuration for collecting Krane-managed deployment logs (local dev)

    data_dir = "/var/lib/vector"

    # Source: Kubernetes Logs
    [sources.kubernetes_logs]
    type = "kubernetes_logs"
    auto_partial_merge = true
    extra_label_selector = "app.kubernetes.io/managed-by=krane,app.kubernetes.io/component=deployment"

    # Filter out init container logs (copy-inject)
    [transforms.filter_init_containers]
    type = "filter"
    inputs = ["kubernetes_logs"]
    condition = '.kubernetes.container_name == "deployment"'

    # Merge multiline logs (stack traces, etc.)
    # Lines starting with whitespace are continuation lines
    [transforms.merge_multiline]
    type = "reduce"
    inputs = ["filter_init_containers"]
    group_by = ["file"]
    expire_after_ms = 2000
    flush_period_ms = 500

      # Start new log entry when line matches common log patterns.
      # Based on Fluent Bit's built-in parsers for Go, Java, Python.
      # See: https://github.com/fluent/fluent-bit/tree/master/src/multiline
      #
      # Matches (NEW LOG ENTRY):
      # - ISO8601 timestamps: 2026-01-27, 2026/01/27
      # - Syslog timestamps: Jan 27 12:34:56, Jan  7 12:34:56
      # - Bracketed time: [12:34:56], [2026-01-27]
      # - Log levels: INFO, ERROR, WARN, DEBUG, FATAL, TRACE, PANIC (with optional []/: prefix/suffix)
      # - JSON objects: {"
      # - Go panics: panic:, http: panic serving
      # - Python tracebacks: Traceback (most recent call last):
      # - Java/C# exceptions: *Exception:, *Error:, Caused by:
      #
      # Does NOT match (CONTINUATION - gets merged):
      # - Indented lines (tabs/spaces)
      # - Stack frames: at com.example..., File "...", goroutine N [...]
      # - Java: ... N more, Suppressed:
      [transforms.merge_multiline.starts_when]
      type = "vrl"
      source = '''
      msg = string!(.message)

      # Timestamp patterns (most common log starters)
      ts_iso = match(msg, r'^\d{4}[-/]\d{2}[-/]\d{2}')
      ts_syslog = match(msg, r'^[A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}')
      ts_bracket = match(msg, r'^\[\d{2}[:\-]\d{2}[:\-]\d{2}')
      ts_time = match(msg, r'^\d{2}:\d{2}:\d{2}[,.\s]')

      # Log level patterns (case insensitive check via lowercase)
      msg_lower = downcase(msg)
      lvl_standard = match(msg_lower, r'^(info|error|warn|warning|debug|fatal|trace|panic|notice|critical|alert|emergency)[\s:\]\[]')
      lvl_bracket = match(msg_lower, r'^\[(info|error|warn|warning|debug|fatal|trace|panic)\]')

      # JSON object start
      json_start = match(msg, r'^\s*\{\"')

      # Language-specific exception starters
      go_panic = match(msg, r'^(panic:|http: panic serving)')
      py_traceback = match(msg, r'^Traceback \(most recent call last\):')
      java_exception = match(msg, r'^[\w.$]+?(Exception|Error|Throwable)[:\s]')
      java_caused = match(msg, r'^Caused by:')

      # Return true if ANY pattern matches (starts new log entry)
      ts_iso || ts_syslog || ts_bracket || ts_time || lvl_standard || lvl_bracket || json_start || go_panic || py_traceback || java_exception || java_caused
      '''

      [transforms.merge_multiline.merge_strategies]
      message = "concat_newline"

    # Extract labels and parse structured logs
    [transforms.extract_labels]
    type = "remap"
    inputs = ["merge_multiline"]
    source = '''
    # Extract Unkey-specific labels from pod (|| for null coalescing)
    .workspace_id = .kubernetes.pod_labels."unkey.com/workspace.id" || ""
    .project_id = .kubernetes.pod_labels."unkey.com/project.id" || ""
    .environment_id = .kubernetes.pod_labels."unkey.com/environment.id" || ""
    .deployment_id = .kubernetes.pod_labels."unkey.com/deployment.id" || ""

    # Keep k8s metadata
    .k8s_pod_name = .kubernetes.pod_name || ""

    # Initialize attributes as empty object
    .attributes = {}

    # Try to parse structured logs (JSON or key=value)
    raw_message = string!(.message)
    parsed, err = parse_json(raw_message)
    if err == null && is_object(parsed) {
        # JSON structured log - keep nested structure
        .message = parsed.msg || parsed.message || raw_message
        .severity = parsed.level || parsed.severity || "info"

        # Store all attributes
        .attributes = parsed
        del(.attributes.msg)
        del(.attributes.message)
        del(.attributes.level)
        del(.attributes.severity)
    } else {
        # Try key=value format (logfmt) - but validate it's real structured data
        parsed, err = parse_key_value(raw_message)
        has_log_keys = exists(parsed.level) || exists(parsed.msg) || exists(parsed.message) || exists(parsed.severity)
        if err == null && is_object(parsed) && has_log_keys {
            .message = parsed.msg || parsed.message || raw_message
            .severity = parsed.level || parsed.severity || "info"

            .attributes = parsed
            del(.attributes.msg)
            del(.attributes.message)
            del(.attributes.level)
            del(.attributes.severity)
        } else {
            # Plain text log - comprehensive severity detection
            # After multiline merge, we scan the ENTIRE content for error indicators
            # Based on: https://betterstack.com/community/guides/logging/log-levels-explained/
            msg_lower = downcase(raw_message)

            # === ERROR DETECTION ===
            # 1. Go panics (anywhere in content)
            go_panic = contains(msg_lower, "panic:") || contains(msg_lower, "panic serving")

            # 2. Python tracebacks
            py_traceback = contains(raw_message, "Traceback (most recent call last):")

            # 3. Java/C# exceptions (Exception/Error/Throwable followed by : or newline)
            java_exception = match(raw_message, r'(Exception|Error|Throwable)[:\r\n]')

            # 4. Explicit ERROR level in log (handles "2026/01/27 12:00:00 ERROR ..." patterns)
            # Match: [ERROR], |ERROR|, " ERROR:", " ERROR ", "ERROR:", at line start
            explicit_error = match(msg_lower, r'(^|[\s\[\|])error([\s:\]\|]|$)') || match(msg_lower, r'(^|[\s\[\|])fatal([\s:\]\|]|$)') || match(msg_lower, r'(^|[\s\[\|])critical([\s:\]\|]|$)')

            # 5. Common error indicators
            error_keywords = contains(msg_lower, "failed to") || contains(msg_lower, "failure:") || contains(msg_lower, "crashed") || contains(msg_lower, "segfault") || contains(msg_lower, "killed") || contains(msg_lower, "oom")

            # === WARN DETECTION ===
            explicit_warn = match(msg_lower, r'(^|[\s\[\|])warn(ing)?([\s:\]\|]|$)')

            # === DEBUG DETECTION ===
            explicit_debug = match(msg_lower, r'(^|[\s\[\|])(debug|trace)([\s:\]\|]|$)')

            # === DETERMINE SEVERITY ===
            if go_panic || py_traceback || java_exception || explicit_error || error_keywords {
                .severity = "error"
            } else if explicit_warn {
                .severity = "warn"
            } else if explicit_debug {
                .severity = "debug"
            } else {
                .severity = "info"
            }
        }
    }

    # Normalize severity (downcase! makes it infallible)
    .severity = downcase!(.severity) || "info"

    # Set region
    .region = "local"

    # Convert timestamp (handle potential error)
    .time, err = to_unix_timestamp(.timestamp, unit: "milliseconds")
    if err != null { .time = to_unix_timestamp(now(), unit: "milliseconds") }

    # Clean up
    del(.kubernetes)
    del(.stream)
    del(.file)
    del(.source_type)
    del(.timestamp)
    '''

    # Deduplicate identical logs (includes attributes for structured logs)
    [transforms.dedupe]
    type = "dedupe"
    inputs = ["extract_labels"]
    fields.match = ["message", "workspace_id", "deployment_id", "severity", "attributes"]
    cache.num_events = 5000

    # Sink: ClickHouse (local)
    [sinks.clickhouse]
    type = "clickhouse"
    inputs = ["dedupe"]
    endpoint = "http://clickhouse:8123"
    database = "default"
    table = "runtime_logs_raw_v1"
    compression = "gzip"
    skip_unknown_fields = true
    healthcheck.enabled = true

    auth.strategy = "basic"
    auth.user = "default"
    auth.password = "password"

      [sinks.clickhouse.batch]
      max_bytes = 5242880
      timeout_secs = 3

      [sinks.clickhouse.encoding]
      timestamp_format = "unix_ms"
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: vector-logs
  namespace: unkey
  labels:
    app: vector-logs
spec:
  selector:
    matchLabels:
      app: vector-logs
  template:
    metadata:
      labels:
        app: vector-logs
    spec:
      serviceAccountName: vector-logs
      containers:
        - name: vector
          image: timberio/vector:0.43.1-alpine
          args: ["--config-dir", "/etc/vector"]
          env:
            - name: VECTOR_LOG
              value: "info"
            - name: VECTOR_SELF_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - name: config
              mountPath: /etc/vector
              readOnly: true
            - name: var-log
              mountPath: /var/log
              readOnly: true
            - name: var-lib-docker
              mountPath: /var/lib/docker
              readOnly: true
            - name: data
              mountPath: /var/lib/vector
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 256Mi
      volumes:
        - name: config
          configMap:
            name: vector-logs-config
        - name: var-log
          hostPath:
            path: /var/log
        - name: var-lib-docker
          hostPath:
            path: /var/lib/docker
        - name: data
          hostPath:
            path: /var/lib/vector
            type: DirectoryOrCreate
      terminationGracePeriodSeconds: 30
