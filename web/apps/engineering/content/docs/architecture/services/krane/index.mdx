---
title: Krane
description: Kubernetes cluster agent for deployment orchestration
---

import { Mermaid } from "@/app/components/mermaid";



Krane is a Kubernetes cluster agent that follows a pull-based model similar to the Kubernetes kubelet. It polls the control plane for state changes using a sequence-based synchronization protocol, applying changes to local Kubernetes resources. This architecture enables multi-cluster orchestration without requiring the control plane to track connected clients or push events.

Krane pulls desired state from ctrl and ensures the actual cluster state matches. It handles deployment and sentinel lifecycle operations (create, update, delete) by translating high-level state messages into Kubernetes resources.

## Architecture

### Pull-Based Model

Krane implements a polling-based architecture where agents in each cluster connect to ctrl's ClusterService via the `Sync` RPC, which establishes a server-streaming connection. The control plane polls its `state_changes` table and streams new entries to connected agents. Krane processes each state change, applies it to Kubernetes, and updates its sequence watermark. On reconnection, Krane sends its last-seen sequence to resume incrementally without missing events.

This model eliminates the need for the control plane to track connected clients in memory, simplifying horizontal scaling and removing a class of connection state bugs.

### Sequence-Based Synchronization

The sync engine uses sequence numbers to track state changes. Every modification to deployments or sentinels is recorded in the `state_changes` table with a monotonically increasing sequence number per region. Krane maintains a `sequenceLastSeen` watermark and polls for changes after that sequence.

On fresh start (sequence=0), Krane receives the complete desired state followed by a `Bookmark` message containing the current maximum sequence. After bootstrap, the stream switches to incremental mode, receiving only new changes as they occur.

State changes are retained for 7 days. If Krane's last-seen sequence falls behind the retention window, it must perform a full bootstrap. This handles long disconnections gracefully while keeping storage bounded.

### Why StatefulSets Instead of Deployments?

We use StatefulSets for stateless containers, which is unusual. The system expects each instance to have a stable DNS address that doesn't change when pods restart.

StatefulSets guarantee this. Each pod gets a predictable name (`dep-abc-0`) and DNS record (`dep-abc-0.dep-abc.unkey.svc.cluster.local`). The control plane tracks these addresses for service discovery and routing.

Standard Deployments use random pod names and changing DNS addresses. This works fine behind a load balancer, but our current architecture needs stable instance addressing.

This is a known design compromise. Future versions might move instance addressing to service meshes instead of requiring stable DNS.

## Deployment Flow

<Mermaid chart={`sequenceDiagram
    autonumber
    participant User
    participant Ctrl as Control Plane
    participant DB as Database
    participant Krane as Krane Agent
    participant K8s as Kubernetes API
    participant Pod

    User->>Ctrl: Create deployment request
    Ctrl->>DB: Store desired state
    Ctrl->>DB: Insert state_change(seq=N)

    Note over Krane: Sync stream polling

    Krane->>Ctrl: (poll for changes > seq)
    Ctrl->>DB: Query state_changes
    Ctrl->>Krane: State(seq=N, ApplyDeployment)

    Krane->>K8s: Create/Update Service
    K8s->>Krane: Service ready
    Krane->>K8s: Create/Update StatefulSet
    K8s->>Krane: StatefulSet created

    K8s->>K8s: Schedule pods
    K8s->>Pod: Pull image & start container
    Pod->>Pod: Container running

    Note over Krane: sequenceLastSeen = N

`} />

## Kubernetes Backend

The Kubernetes backend runs inside a cluster with appropriate RBAC permissions. It uses in-cluster config to authenticate with the API server.

These need to be fine tuned, but they work for now.

```yaml
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

### Resource Creation

Creating a deployment creates two resources:

**Headless Service** with `ClusterIP: None` and `publishNotReadyAddresses: true` for DNS-based discovery. Each pod gets a DNS record even before it's ready. The Service selector matches `unkey.deployment.id`.

**StatefulSet** with the specified replicas, CPU, and memory. Resource requests and limits are set to the same value for predictable scheduling. Image pull secrets are automatically added for Depot registry images. Restart policy is always.

Resource limits are enforced at the pod level. Exceeding memory kills the pod. Exceeding CPU throttles it.

## RBAC Requirements

The Kubernetes backend requires specific RBAC permissions to function. Krane needs the ability to create, read, and delete StatefulSets in the Apps API group, create, read, and delete Services in the Core API group, and list and read Pods in the Core API group to query status.

A typical RBAC configuration looks like this:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: krane
  namespace: unkey
rules:
  - apiGroups: ["apps"]
    resources: ["statefulsets"]
    verbs: ["create", "get", "list", "delete"]
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["create", "get", "delete"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
```

Without these permissions, Krane cannot manage deployments and will return permission denied errors.

## Labels and Management

All resources created by Krane are labeled with `unkey.managed.by=krane` and `unkey.deployment.id=\{deployment_id\}`. These labels serve multiple purposes: they identify resources managed by Krane for filtering and querying, they enable automatic cleanup during eviction scans, and they prevent Krane from interfering with non-Krane resources in shared namespaces.

When querying deployments, Krane verifies the `unkey.managed.by` label matches `krane`. This prevents it from returning information about deployments created by other tools or controllers in the same namespace.


### ClusterService API

The control plane exposes a `ClusterService` with these key RPCs:

**Sync** establishes a server-streaming connection for receiving state changes. Krane sends its region and last-seen sequence number; the control plane streams bootstrap state (if sequence=0) followed by incremental changes. The control plane polls its `state_changes` table and streams new entries as they appear.

**GetDesiredDeploymentState** and **GetDesiredSentinelState** return the current desired state for a specific resource. Used for on-demand reconciliation when Kubernetes reports unexpected state.

**UpdateDeploymentState** and **UpdateSentinelState** receive status updates from Krane about actual Kubernetes state (pod running, pod failed, etc.).

### State Change Distribution

When deployment changes occur, the control plane stores the desired state in the database and inserts a row into `state_changes` with the resource ID, operation type (upsert/delete), and region. Krane instances polling for that region receive the change on their next poll cycle. Each Krane instance independently applies changes to its local cluster.

### Multi-Region Support

The `state_changes` table is partitioned by region, so each Krane instance only receives changes relevant to its cluster. The control plane doesn't need to know which Krane instances exist or are connected; it simply writes changes to the database, and any Krane polling that region will receive them.
