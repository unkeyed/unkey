"var Component=(()=>{var g=Object.create;var s=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var i in e)s(t,i,{get:e[i],enumerable:!0})},o=(t,e,i,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!w.call(t,a)&&a!==i&&s(t,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return t};var b=(t,e,i)=>(i=t!=null?g(u(t)):{},o(e||!t||!t.__esModule?s(i,\"default\",{value:t,enumerable:!0}):i,t)),k=t=>o(s({},\"__esModule\",{value:!0}),t);var h=f((A,c)=>{c.exports=_jsx_runtime});var I={};y(I,{default:()=>d});var n=b(h());function l(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",li:\"li\",ol:\"ol\",p:\"p\",pre:\"pre\",...t.components},{Image:i}=e;return i||v(\"Image\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:`Large language models are getting faster and cheaper. The below charts show progress in OpenAI's GPT family of models over the past\nyear:`}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Cost per million tokens ($)\"})}),`\n`,(0,n.jsx)(i,{src:\"/images/blog-images/semantic-caching/cost-per-token-2.png\",alt:\"Chart of cost per token over time\",width:\"1920\",height:\"1080\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Tokens per second\"})}),`\n`,(0,n.jsx)(i,{src:\"/images/blog-images/semantic-caching/tokens-per-second-2.png\",alt:\"Chart of cost per token over time\",width:\"1920\",height:\"1080\"}),`\n`,(0,n.jsx)(e.p,{children:\"Recent releases like Meta's Llama 3 and Gemini Flash have pushed the cost / speed frontier further:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Below data is for May 2024\"})}),`\n`,(0,n.jsx)(i,{src:\"/images/blog-images/semantic-caching/models-comparison.png\",alt:\"Chart comparing cost and speed of LLMs in May 2024\",width:\"1920\",height:\"1080\"}),`\n`,(0,n.jsx)(e.p,{children:\"As cost and latency have decreased, more complex LLM workflows have become increasingly viable, which in turn bring their own set of challenges.\"}),`\n`,(0,n.jsx)(i,{src:\"/images/blog-images/semantic-caching/llm-agent-tweet.png\",alt:\"Chart comparing cost and speed of LLMs in May 2024\",width:\"1920\",height:\"1080\"}),`\n`,(0,n.jsxs)(e.p,{children:[`Since these workflows often involve multiple steps and retries, taking measures to reduce cost and latency remains helpful. One way to do\nso is via `,(0,n.jsx)(e.em,{children:\"semantic caching\"}),\".\"]}),`\n`,(0,n.jsx)(e.h2,{id:\"example-use-case-rag\",children:\"Example use case: RAG\"}),`\n`,(0,n.jsx)(e.p,{children:`Say that you want to build a RAG (retrieval-augmented generation) chatbot for customer support. It can respond to user queries and suggest\narticles from your support center.`}),`\n`,(0,n.jsx)(e.p,{children:\"This workflow would involve multiple API calls:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Generate embedding of user query\"}),`\n`,(0,n.jsx)(e.li,{children:\"Search vector DB for relevant embeddings and append to prompt\"}),`\n`,(0,n.jsx)(e.li,{children:\"Query LLM API with enhanced prompt\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:`Through caching, you could avoid doing this work except for when users ask questions that haven't been asked before - otherwise, re-use\nexisting work.`}),`\n`,(0,n.jsx)(e.h2,{id:\"caching\",children:\"Caching\"}),`\n`,(0,n.jsx)(e.p,{children:\"A simple caching solution would be to cache results using the user query as the key, and the result of the workflow as the value.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This would enable re-use of responses between identical queries. But it would rely on user queries being phrased identically.\"}),`\n`,(0,n.jsx)(e.p,{children:`For instance, if two users asked \"How do I cancel my subscription\" then the second would be served the cached response. But if another\nuser was to ask \"I need to cancel my subscription - how?\" then we'd see a cache miss. The question is the same in terms of its intent,\nbut the phrasing is different.`}),`\n`,(0,n.jsx)(e.p,{children:`This is how semantic caching can be useful: through caching based on the embedding of the query, we can ensure that all users who ask\nthe same question receive a cache hit. The below diagrams give an overview of the architecture:`}),`\n`,(0,n.jsx)(i,{src:\"/images/blog-images/semantic-caching/cachehit.png\",alt:\"Diagram showing cache hit\",width:\"1920\",height:\"1080\"}),`\n`,(0,n.jsx)(i,{src:\"/images/blog-images/semantic-caching/cachemiss.png\",alt:\"Diagram showing cache miss\",width:\"1920\",height:\"1080\"}),`\n`,(0,n.jsx)(e.h2,{id:\"try-it-now\",children:\"Try it now\"}),`\n`,(0,n.jsxs)(e.p,{children:[`In response to feedback from our AI customers, we're offering semantic caching now as part of Unkey. You can enable it now through\n`,(0,n.jsx)(e.a,{href:\"https://unkey.dev/semantic-caching\",children:\"signing up\"}),\" and changing the baseUrl parameter of the OpenAI SDK:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-jsx\",children:`const openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: \"https://<gateway>.llm.unkey.io\", // change the baseUrl parameter to your gateway name\n});\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Unkey's semantic caching is free, supports streaming, and comes with built-in observability and analytics. In future we will offer deeper integration with Unkey's API keys for enhanced security and performance.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you'd like to read more about how it works, check out our \",(0,n.jsx)(e.a,{href:\"https://unkey.dev/docs/semantic-caching\",children:\"documentation\"}),\".\"]})]})}function d(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(l,{...t})}):l(t)}function v(t,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+t+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return k(I);})();\n;return Component;"