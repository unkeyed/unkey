---
openapi: post /v2/analytics.getVerifications
---

<Info>
  Query your key verification data using SQL with automatic workspace isolation and security controls. Choose from raw events or pre-aggregated tables for optimal performance.
</Info>

## Available Tables

<CardGroup cols={2}>
  <Card title="Raw Events" icon="database">
    **key_verifications** - Individual events (1 month TTL)

    Best for: Recent data, full event details
  </Card>
  <Card title="Minute Aggregates" icon="clock">
    **key_verifications_per_minute** - Pre-aggregated (7 day TTL)

    Best for: Recent detailed analysis
  </Card>
  <Card title="Hour Aggregates" icon="chart-line">
    **key_verifications_per_hour** - Pre-aggregated (30 day TTL)

    Best for: Weekly/monthly trends
  </Card>
  <Card title="Day Aggregates" icon="calendar">
    **key_verifications_per_day** - Pre-aggregated (100 day TTL)

    Best for: Long-term analysis
  </Card>
</CardGroup>

## Schema Reference

### Raw Events Table

**`key_verifications`** - Individual verification events with complete details. Time is stored as **Int64 unix milliseconds**.

| Column | Type | Description |
|--------|------|-------------|
| `request_id` | String | Unique request identifier |
| `time` | Int64 | Unix timestamp in milliseconds |
| `workspace_id` | String | Workspace identifier (auto-filtered) |
| `key_space_id` | String | API identifier |
| `identity_id` | String | Identity identifier (empty if none) |
| `key_id` | String | Key identifier |
| `region` | String | Region code (e.g., `us-east-1`) |
| `outcome` | String | Verification result |
| `tags` | Array(String) | Tags associated with verification |
| `spent_credits` | Int64 | Credits consumed (0 if none) |
| `latency` | Float64 | Latency in milliseconds |

### Aggregated Tables Schema

<Warning>
  Aggregated tables require merge functions: `avgMerge()`, `quantilesTDigestMerge()`, `sum()`
</Warning>

| Column | Type | How to Query |
|--------|------|-----------------|
| `time` | DateTime/Date | `WHERE time >= now() - INTERVAL 7 DAY` |
| `workspace_id` | String | Auto-filtered |
| `key_space_id` | String | Use `apiId` virtual column |
| `identity_id` | String | Use `externalId` virtual column |
| `key_id` | String | Direct filtering |
| `outcome` | String | Direct filtering |
| `tags` | Array(String) | Use `has(tags, 'value')` |
| `count` | SimpleAggregateFunction | `sum(count)` |
| `spent_credits` | SimpleAggregateFunction | `sum(spent_credits)` |
| `latency_avg` | AggregateFunction | `avgMerge(latency_avg)` |
| `latency_p75` | AggregateFunction | `quantilesTDigestMerge(0.75)(latency_p75)[1]` |
| `latency_p99` | AggregateFunction | `quantilesTDigestMerge(0.99)(latency_p99)[1]` |

## Query Examples

<AccordionGroup>
  <Accordion title="Basic Count">
    ```sql
    SELECT COUNT(*) as total
    FROM key_verifications
    WHERE time >= (toUnixTimestamp(now()) - 7*24*3600) * 1000
    ```
  </Accordion>

  <Accordion title="Success Rate">
    ```sql
    SELECT
      countIf(outcome = 'VALID') as successful,
      COUNT(*) as total,
      (successful / total) * 100 as success_rate
    FROM key_verifications
    WHERE time >= (toUnixTimestamp(now()) - 24*3600) * 1000
    ```
  </Accordion>

  <Accordion title="Hourly Time Series with Latency">
    ```sql
    SELECT
      toStartOfHour(time) as hour,
      sum(count) as total_requests,
      avgMerge(latency_avg) as avg_latency_ms,
      quantilesTDigestMerge(0.99)(latency_p99)[1] as p99_latency_ms
    FROM key_verifications_per_hour
    WHERE time >= now() - INTERVAL 7 DAY
    GROUP BY hour
    ORDER BY hour
    ```
  </Accordion>

  <Accordion title="Top Keys by Usage">
    ```sql
    SELECT
      key_id,
      COUNT(*) as requests
    FROM key_verifications
    WHERE time >= (toUnixTimestamp(now()) - 7*24*3600) * 1000
    GROUP BY key_id
    ORDER BY requests DESC
    LIMIT 10
    ```
  </Accordion>

  <Accordion title="Filter by API (Virtual Column)">
    ```sql
    SELECT
      key_space_id,
      sum(count) as total
    FROM key_verifications_per_hour
    WHERE apiId IN ('api_123', 'api_456')
      AND time >= now() - INTERVAL 30 DAY
    GROUP BY key_space_id
    ORDER BY total DESC
    ```
  </Accordion>

  <Accordion title="Credits Consumed by Key">
    ```sql
    SELECT
      key_id,
      sum(spent_credits) as total_credits
    FROM key_verifications_per_day
    WHERE time >= now() - INTERVAL 30 DAY
    GROUP BY key_id
    ORDER BY total_credits DESC
    LIMIT 20
    ```
  </Accordion>

  <Accordion title="Error Rate Analysis">
    ```sql
    SELECT
      key_id,
      sum(count) as total,
      sumIf(count, outcome != 'VALID') as errors,
      (errors / total) * 100 as error_rate
    FROM key_verifications_per_hour
    WHERE time >= now() - INTERVAL 7 DAY
    GROUP BY key_id
    HAVING error_rate > 5
    ORDER BY error_rate DESC
    ```
  </Accordion>

  <Accordion title="Regional Analysis">
    ```sql
    SELECT
      region,
      sum(count) as requests,
      avgMerge(latency_avg) as avg_latency
    FROM key_verifications_per_hour
    WHERE time >= now() - INTERVAL 7 DAY
    GROUP BY region
    ORDER BY requests DESC
    ```
  </Accordion>

  <Accordion title="Tag Filtering">
    ```sql
    SELECT
      key_id,
      COUNT(*) as count
    FROM key_verifications
    WHERE has(tags, 'production')
      AND time >= (toUnixTimestamp(now()) - 7*24*3600) * 1000
    GROUP BY key_id
    ORDER BY count DESC
    ```
  </Accordion>
</AccordionGroup>

## Time Filtering

<Tabs>
  <Tab title="Raw Table">
    Time is stored as **Int64 unix milliseconds**.

    ```sql
    -- Last 24 hours
    WHERE time >= (toUnixTimestamp(now()) - 24*3600) * 1000

    -- Last 7 days
    WHERE time >= (toUnixTimestamp(now()) - 7*24*3600) * 1000

    -- Specific range
    WHERE time >= toUnixTimestamp(toDateTime('2024-01-01')) * 1000
      AND time < toUnixTimestamp(toDateTime('2024-02-01')) * 1000
    ```
  </Tab>

  <Tab title="Aggregated Tables">
    Time is stored as **DateTime** or **Date**.

    ```sql
    -- Last 24 hours
    WHERE time >= now() - INTERVAL 24 HOUR

    -- Last 7 days
    WHERE time >= now() - INTERVAL 7 DAY

    -- Specific range
    WHERE time >= toDateTime('2024-01-01')
      AND time < toDateTime('2024-02-01')
    ```
  </Tab>
</Tabs>

## Outcome Values

| Outcome | Description |
|---------|-------------|
| `VALID` | Key verification succeeded |
| `RATE_LIMITED` | Request exceeded rate limit |
| `EXPIRED` | Key has expired |
| `DISABLED` | Key is disabled |
| `INSUFFICIENT_PERMISSIONS` | Key lacks required permissions |
| `FORBIDDEN` | Access forbidden |
| `USAGE_EXCEEDED` | Usage limit exceeded |
| `NOT_FOUND` | Key not found |
| `INVALID` | Key is invalid |

## Performance Tips

<Steps>
  <Step title="Choose the Right Table">
    - Use **raw table** for recent data (hours/days)
    - Use **aggregated tables** for longer periods (weeks/months)
  </Step>

  <Step title="Always Filter by Time">
    ClickHouse is optimized for time-series data. Always include time filters to improve query performance.
  </Step>

  <Step title="Use LIMIT Clauses">
    Prevent large result sets by adding `LIMIT` to your queries (max 10,000 rows).
  </Step>

  <Step title="Leverage Aggregations">
    Use `GROUP BY` and aggregate functions in SQL rather than processing in application code.
  </Step>
</Steps>

## Limitations

| Limit | Value |
|-------|-------|
| **Query Timeout** | Maximum execution time: 30 seconds |
| **Result Size** | Maximum result rows: 10,000 (use LIMIT) |
| **Query Types** | Only SELECT queries allowed |
| **Blocked Operations** | INSERT, UPDATE, DELETE, system tables, dangerous functions |
| **Workspace Isolation** | Automatically enforced (workspace_id filter injected) |
