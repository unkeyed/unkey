---
title: Distributed cache invalidation
description: Ensure cache consistency across multiple API nodes using Kafka-based event streaming
---

Unkey's distributed cache invalidation system ensures cache consistency across multiple API nodes in horizontally scaled deployments. When one node updates or deletes cached data, all other nodes are automatically notified to invalidate their cached copies, preventing stale data issues.

## Overview

In a multi-node API deployment, cache inconsistency can occur when:
- Node A deletes an API key, but Node B still has it cached and continues to authorize requests
- Node A updates user permissions, but Node B serves outdated authorization decisions  
- Database writes happen on one node while other nodes serve cached responses

The distributed cache invalidation system solves this by using Kafka as an event streaming platform to broadcast cache invalidation events across all cluster nodes.

## How it works

### Event flow architecture

```
Node A: API Delete → ClusterCache.SetNull() → Kafka Event → [Node B, Node C, ...]
Node B: Receives Event → LocalCache.Remove() → Fresh data on next request
```

### Key components

1. **Kafka-based event streaming**: Cache changes trigger events that notify all cluster nodes
2. **ClusterCache wrapper system**: Drop-in replacement for existing cache implementations
3. **Automatic event production**: `Set()` and `SetNull()` operations automatically broadcast invalidation events
4. **Smart consumption**: Nodes only process invalidation events from other instances
5. **Debug headers**: Real-time cache monitoring via `X-Unkey-Debug-Cache` headers

## Configuration

### Environment variables

Configure Kafka brokers for distributed cache invalidation:

```bash
# Required: Comma-separated list of Kafka broker addresses
UNKEY_KAFKA_BROKERS="kafka1:9092,kafka2:9092,kafka3:9092"

# Optional: Kafka topic name (defaults to "cache-invalidations")
UNKEY_CACHE_INVALIDATION_TOPIC="cache-invalidations"
```

### Docker Compose setup

Add Kafka to your `docker-compose.yaml`:

```yaml
services:
  kafka:
    container_name: kafka
    image: bitnami/kafka:latest
    ports:
      - 9092:9092
    environment:
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@localhost:9093
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: true

  api:
    # ... other configuration
    environment:
      UNKEY_KAFKA_BROKERS: "kafka:9092"
    depends_on:
      kafka:
        condition: service_started
```

## Cache debug headers

The system provides real-time cache monitoring through structured HTTP response headers:

### Header format

```
X-Unkey-Debug-Cache: cache_name:latency:status
```

### Example headers

```
X-Unkey-Debug-Cache: api_by_id:2.5ms:FRESH
X-Unkey-Debug-Cache: verification_key_by_hash:1.2ms:STALE
X-Unkey-Debug-Cache: ratelimit_namespace:0.8ms:MISS
```

### Cache statuses

- **FRESH**: Data is current and within freshness window
- **STALE**: Data exists but is beyond freshness window (served while revalidating)
- **MISS**: No cached data found, fetched from origin
- **ERROR**: Cache operation failed

## Deployment considerations

### Gradual rollout

The system is designed to be optional - if no Kafka brokers are configured, it falls back to local-only caching. This allows for gradual rollout and testing:

1. **Phase 1**: Deploy without Kafka configuration (local caching only)
2. **Phase 2**: Add Kafka infrastructure 
3. **Phase 3**: Enable distributed invalidation by setting `UNKEY_KAFKA_BROKERS`

### Production recommendations

For production deployments:

1. **Use multiple Kafka brokers** for high availability
2. **Configure proper replication factor** (minimum 3 for production)
3. **Set up monitoring** for Kafka cluster health
4. **Disable auto topic creation** in production environments
5. **Configure authentication and encryption** for Kafka communication

### Performance impact

- **Minimal latency overhead**: Cache invalidation is non-blocking
- **Reliable delivery**: Uses Kafka's built-in durability guarantees
- **Cluster-aware**: Each node has unique instance ID to avoid self-invalidation loops
- **Failure resilience**: Cache invalidation failures never crash API requests

## Monitoring and observability

### Debug headers

Monitor cache performance using the `X-Unkey-Debug-Cache` headers in API responses:

```bash
curl -H "Authorization: Bearer your-key" \
     https://api.unkey.com/v2/apis.getApi \
     -d '{"apiId": "api_123"}' \
     -v
```

Look for headers like:
```
X-Unkey-Debug-Cache: api_by_id:1.5ms:FRESH
```

### Metrics and logging

The system logs important events:

- Cache invalidation event production
- Event consumption and processing
- Kafka connection status
- Cache hit/miss rates and latencies

## Troubleshooting

### Common issues

**Cache invalidation not working**
- Verify Kafka brokers are accessible from all nodes
- Check that `UNKEY_KAFKA_BROKERS` is set correctly
- Ensure topic exists and has proper permissions

**High cache miss rates**
- May indicate over-aggressive invalidation
- Check debug headers to understand cache behavior
- Monitor Kafka event volume

**Kafka connection failures**
- Verify broker addresses and network connectivity
- Check Kafka cluster health
- Review authentication/authorization settings

### Debug commands

Check Kafka topic status:
```bash
kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic cache-invalidations
```

Monitor invalidation events:
```bash
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic cache-invalidations --from-beginning
```

## Implementation details

### Event structure

Cache invalidation events use the following protobuf structure:

```protobuf
message CacheInvalidationEvent {
  string cache_name = 1;      // Name of the cache to invalidate
  string cache_key = 2;       // Specific key to invalidate
  int64 timestamp = 3;        // Unix millisecond timestamp
  string source_instance = 4; // Node that triggered invalidation
}
```

### Supported cache operations

The following cache operations trigger distributed invalidation:

- `Set(key, value)`: Updates cache and broadcasts invalidation
- `SetNull(key)`: Marks key as deleted and broadcasts invalidation  
- `Remove(keys...)`: Removes keys and broadcasts invalidation

### SWR compatibility

The system works seamlessly with existing Stale-While-Revalidate (SWR) patterns:

- Cache hits return immediately with appropriate status
- Stale data is served while background revalidation occurs
- Invalidation events trigger immediate cache removal
- Next request fetches fresh data from origin

## Security considerations

- **Network isolation**: Ensure Kafka brokers are only accessible to authorized nodes
- **Authentication**: Configure SASL/SCRAM or mTLS for Kafka authentication
- **Encryption**: Use TLS for Kafka communication in production
- **Topic permissions**: Restrict topic access to prevent unauthorized invalidation events

The distributed cache invalidation system provides a robust foundation for maintaining cache consistency in Unkey's horizontally scaled architecture while preserving performance and reliability.